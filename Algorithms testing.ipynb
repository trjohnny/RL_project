{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af392847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import panda_gym\n",
    "from agents.on_policy.a2c_agent import A2CAgent\n",
    "from agents.off_policy.ddpg_agent import DDPGAgent\n",
    "from agents.on_policy.a2c_discrete_agent import A2CDiscreteAgent\n",
    "from agents.on_policy.a2c_n_step_ahead_agent import A2CNStepAheadAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"PandaPushDense-v3\")\n",
    "\n",
    "episodes = 40000\n",
    "verbose = 2\n",
    "\n",
    "num_states = env.observation_space['observation'].shape[0] + env.observation_space['desired_goal'].shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "a2c_agent = A2CAgent(num_states, num_actions, lower_bound, upper_bound)\n",
    "\n",
    "rewards_a2c = a2c_agent.train_agent(env, episodes, verbose)\n",
    "\n",
    "discrete_agent = A2CDiscreteAgent(num_states, num_actions, lower_bound, upper_bound)\n",
    "\n",
    "rewards_discrete = discrete_agent.train_agent(env, episodes, verbose)\n",
    "\n",
    "ddpg_agent = DDPGAgent(num_states, num_actions, lower_bound, upper_bound) \n",
    "\n",
    "rewards_ddpg = ddpg_agent.train_agent(env, episodes, verbose)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_rewards_with_average(rewards_list, colors, labels_list, window_size=100):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    for i, rewards in enumerate(rewards_list):\n",
    "        rewards_series = pd.Series(rewards)\n",
    "        avg_rewards = rewards_series.rolling(window=window_size, min_periods=1).mean()\n",
    "        plt.plot(range(len(rewards)), avg_rewards, color=colors[i], label=labels_list[i])\n",
    "        \n",
    "        indexes = [i * 3001 for i in range(int(len(rewards_ddpg)/3001+1))]\n",
    "        indexes.append(len(avg_rewards)-1)\n",
    "        val = [avg_rewards[k] for k in indexes]\n",
    "        \n",
    "        for ind, v in zip(indexes, val):\n",
    "            plt.annotate(f'{v:.2f}', (ind, v),\n",
    "                         textcoords=\"offset points\",\n",
    "                         xytext=(0,20),\n",
    "                         ha='center',\n",
    "                         color=colors[i],\n",
    "                         arrowprops=dict(arrowstyle='->', color=colors[i]))\n",
    "\n",
    "\n",
    "    plt.title('Rewards and Rolling Average ({})'.format(window_size))\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Now you can use this function like this:\n",
    "colors = ['blue', 'orange', 'red', 'green', 'purple']\n",
    "labels_list = ['A2C', 'Discrete A2C', 'DDPG']\n",
    "rewards_list = [rewards_a2c, rewards_discrete, rewards_ddpg]\n",
    "plot_rewards_with_average(rewards_list, colors, labels_list, 3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}